\documentclass[]{article}
\usepackage{listings}

%opening
\title{TDDD41 - Lab 2}
\author{Martin Estgren, Alice Reinaudo}

\begin{document}

\maketitle

\section{Data and binning}
%Dataset, bins
For this assignment, we use the iris dataset, which contains information about sizes of sepals and petals for three species of iris. With 3 bins, looking at the visualizations for the attribute values, it appears that the sepal width and length have moderate to low impact, while the petal width and length have high impact on the actual type of flower. This is because values for the sepal dimensions tend to overlap more between different species of iris while the petal dimensions do not. Similar information can be seen regardless of the number of bins, although the less bins we have the more information is lost.

Since we discretize and choose the number of bins, the more small bins we have the less often each of the corresponding discretized values will appear, and the smaller the support that each attribute-value set has on average. Vice versa, if the bins are few and wide then each value can repeat more often, and so the support will be on average higher. We keep this into account for our minimum support.

\section{k-Means}
%Kmeans
The results with k-means are quite good with 3 bins and 3 clusters. From the confusion matrix we can see that all the iris setosa items are classified correctly, while versicolor and virginica sometimes get swapped. If we increase the number of bins, we could expect our results to improve or at worst stay unchanged, since we would have a more precise measure of the distances of the data points.

\begin{lstlisting}
Class attribute: class
Classes to Clusters:

  0  1  2  <-- assigned to cluster
  0  0 50 | Iris-setosa
 48  2  0 | Iris-versicolor
  7 43  0 | Iris-virginica

Cluster 0 <-- Iris-versicolor
Cluster 1 <-- Iris-virginica
Cluster 2 <-- Iris-setosa

Incorrectly clustered instances :	9.0	  6      %
\end{lstlisting}

\section{Association analysis}
% Association analysis
The frequent itemsets in this case correspond to the frequent attribute-value sets in our database, and a transaction corresponds to a single data point.

\section{Clustering through association analysis}
By using the default settings for the Apriori algorithm, we do not get enough rules to cover all clusters. Instead we used $numRules=1000$, $metricType=confidence$ and $minMetric=0.9$. In this way we obtain 1000 rules and we keep what has more than 90\% confidence. The following are the interesting rules that we got.
\begin{lstlisting}
6. petallength='(-inf-2.966667]' 50 							==> cluster=cluster3 50 	<conf:(1)> 
10. petalwidth='(-inf-0.9]' 50 									==> cluster=cluster3 50  	<conf:(1)> 
51. petallength='(2.966667-4.933333]' petalwidth='(0.9-1.7]' 48 ==> cluster=cluster1 48  	<conf:(1)> 
277. sepallength='(5.5-6.7]' petallength='(2.966667-4.933333]' petalwidth='(0.9-1.7]' 33 ==> cluster=cluster1 33    <conf:(1)> 
106. petallength='(4.933333-inf)' petalwidth='(1.7-inf)' 40 	==> cluster=cluster2 40     <conf:(1)> 
284. sepalwidth='(2.8-3.6]' petalwidth='(1.7-inf)' 29 			==> cluster=cluster2 29   	<conf:(1)> 
\end{lstlisting}
The reason that we do not want to have the class attribute in the antecedent is that it is what we want to predict to begin with, so when we have a new flower to consider we do not have its class at our disposal. Moreover we only want the cluster attribute in the consequent, because the other attributes we want to use to predict, not to be predicted.

\section{Experiments}

\subsection{Different number of bins}
Increasing the number of bins can help take more information into account and calculating distances more precisely in k-means, and obtain more rules in association analysis. On the other hand, too many clusters could provide little extra information and possibly confuse the algorithms. 

We tried using 6 bins. For k-means, we actually obtain a perfect result with no misclassified elements.

We obtain the following rules:
\begin{lstlisting}
3. 	petallength='(-inf-1.983333]' 50 							==> cluster=cluster1 50    <conf:(1)>
12. petalwidth='(-inf-0.5]' 49 									==> cluster=cluster1 49    <conf:(1)> 
61. sepallength='(5.5-6.1]' petallength='(3.95-4.933333]' 21 	==> cluster=cluster2 21    <conf:(1)>
132.petallength='(3.95-4.933333]' petalwidth='(0.9-1.3]' 18 	==> cluster=cluster2 18    <conf:(1)> 
75. sepallength='(6.1-6.7]' petallength='(4.933333-5.916667]' 20==> cluster=cluster3 20    <conf:(1)>
127.sepalwidth='(2.8-3.2]' petallength='(4.933333-5.916667]' 18 ==> cluster=cluster3 18    <conf:(1)>
\end{lstlisting}
From these we can see that the first cluster is mostly characterized by the dimensions of the petals, while the second and third clusters are characterized by a mix of attribute values. The clustering however is not so good, because 31.3\% of the flowers are in the incorrect cluster. This is a worse result, so it seems that we 

\subsection{Different number of clusters}
We do not necessarily need to have as many clusters as classes, but if we want to have a reasonable chance of correctly predicting class labels from clusters, we need at least as many clusters as classes. If we have more clusters, it is then not a problem to have more than one cluster map to the same class. These may for instance correspond to further subdivisions of flower types. 

We experimented with 6 clusters, twice as many as the classes. What happened is that rules satisfying our previously stated constraints were only found for 3 of the clusters. The other clusters also contained points, but just a few so that the minimum confidence was not satisfied, because most other points belonging to the same classes were in the larger clusters. From this we can conclude that the classes are well separable and that 3 clusters are good enough to obtain an accurate classification.

\begin{lstlisting}
 690. sepalwidth='(2.8-3.6]' petallength='(4.933333-inf)' 28 		==> cluster=cluster2 26    <conf:(0.93)>
 709. sepalwidth='(2.8-3.6]' petallength='(2.966667-4.933333]' 24 	==> cluster=cluster1 22    <conf:(0.92)> 
 726. sepalwidth='(2.8-3.6]' petallength='(2.966667-4.933333]' petalwidth='(0.9-1.7]' 21 ==> cluster=cluster1 19    <conf:(0.9)>
\end{lstlisting}

\end{document}
