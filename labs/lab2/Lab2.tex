\documentclass[]{article}
\usepackage{listings}

%opening
\title{TDDD41 - Lab 2}
\author{Martin Estgren, Alice Reinaudo}

\begin{document}

\maketitle

\section{Data and binning}
%Dataset, bins
For this assignment, we use the iris dataset, which contains information about sizes of sepals and petals for three species of iris. With 3 bins, looking at the visualizations for the attribute values, it appears that the sepal width and length have moderate to low impact, while the petal width and length have high impact on the actual type of flower. This seems to occur because values for the sepal dimensions tend to overlap more between different species of iris while the petal dimensions do not. Similar information can be seen regardless of the number of bins, although the less bins we have the more information is lost.

Since we discretize and choose the number of bins, the more small bins we have the less often each of the corresponding discretized values will appear, and the smaller the support that each attribute-value set has on average. Vice versa, if the bins are few and wide then each value can repeat more often, and so the support will be on average higher. We keep this into account especially with regards to the minimum support.

\section{Parameters}
The following are the parameters we used for the functions, unless otherwise specified e.g. in the experiments:
\begin{itemize}
\item Minimum support = 0.1
\item Metric type: confidence
\item Minimum confidence = 0.9
\end{itemize}

\section{k-Means}
%Kmeans
The results with k-means are quite good with 3 bins and 3 clusters. From the confusion matrix we can see that all the iris setosa items are classified correctly, while versicolor and virginica sometimes get swapped. If we increase the number of bins, we could expect our results to improve or at worst stay unchanged, since we would have a more precise measure of the distances of the data points.

\begin{lstlisting}
Class attribute: class
Classes to Clusters:

  0  1  2  <-- assigned to cluster
  0  0 50 | Iris-setosa
 48  2  0 | Iris-versicolor
  7 43  0 | Iris-virginica

Cluster 0 <-- Iris-versicolor
Cluster 1 <-- Iris-virginica
Cluster 2 <-- Iris-setosa

Incorrectly clustered instances :	9.0	  6      %
\end{lstlisting}

\section{Association analysis}
% Association analysis
The frequent itemsets in this case correspond to the frequent attribute-value sets in our database, and a transaction corresponds to a single data point.

\section{Clustering through association analysis}
By using the default settings for the A priori algorithm, we do not get enough rules to cover all clusters. Instead we used $numRules=1000$, $metricType=confidence$ and $minMetric=0.9$. In this way we obtain 1000 rules and we keep what has more than 90\% confidence. The following are the interesting rules that we got.
\begin{table}[]
\centering
\caption{Association rules}
\label{my-label}
\begin{tabular}{llll}
\hline
Parameters & Cluster & Occurences & Confidience \\ \hline
PL='(-inf-2.96]'  & \vline  3 & \vline  50 & \vline  1 \\ \hline
PW='(-inf-0.9]' PL='(2.96-4.93]',PL='(2.96-4.93]' & \vline  3 & \vline  50 & \vline  1 \\ \hline
PW='(0.9-1.7]' SL='(5.5-6.7]' & \vline  1 & \vline 48 & \vline 1 \\ \hline
PW='(0.9-1.7]' PL='(4.93-inf)' & \vline  1 & \vline 33 & \vline 1 \\ \hline
PW='(1.7-inf)' SW='(2.8-3.6]' & \vline  2 & \vline  40 & \vline  1 \\ \hline
PW='(1.7-inf)' &  \vline  2 & \vline  29 & \vline  1 \\ \hline
\end{tabular}
\end{table}

The reason that we do not want to have the class attribute in the antecedent is that it is what we want to predict to begin with, so when we have a new flower to consider we do not have its class at our disposal. Moreover we only want the cluster attribute in the consequent, because the other attributes we want to use to predict, not to be predicted.

\section{Experiments}

\subsection{Different number of bins}
Increasing the number of bins can help take more information into account and calculating distances more precisely in k-means, and obtain more rules in association analysis. On the other hand, too many clusters could provide little extra information and possibly confuse the algorithms. 

We tried using 6 bins. For k-means, we actually obtain a perfect result with no misclassified elements, which is consistent with our assumption in the previous paragraph.

We obtain the following rules. 
\begin{table}[]
\centering
\caption{Association rules}
\label{my-label}
\begin{tabular}{llll}
\hline
Parameters & Cluster & Occurences & Confidience \\ \hline
PL='(-inf-1.983]' &\vline 1 &\vline 50 &\vline 1 \\ \hline
PW='(-inf-0.5]'  &\vline 1 &\vline  49  &\vline 1 \\ \hline
SL='(5.5-6.1]' PL='(3.95-4.93]' &\vline 2 &\vline 21 &\vline 1 \\ \hline
PL='(3.95-4.93]' PW='(0.9-1.3]' &\vline 2 &\vline 18 &\vline 1 \\ \hline
SL='(6.1-6.7]' PL='(4.93-5.916]' &\vline 3 &\vline 20  &\vline 1 \\ \hline
SW='(2.8-3.2]' PL='(4.93-5.916]' &\vline 3  &\vline 18  &\vline 1 \\ \hline
\end{tabular}
\end{table}
From these we can see that the first cluster is mostly characterized by the dimensions of the petals, while the second and third clusters are characterized by a mix of attribute values. The clustering however is not so good, because 31.3\% of the flowers are in the incorrect cluster. It could be that, because we have a higher "resolution" in the possible values that the attributes can take, some sets of attribute values do not reach minimum support and don't give rise to the correct rules. This highlights that it may be important not to overdo the binning process but instead we try to make the discretization in a more significant way, for example by domain knowledge.

\subsection{Different number of clusters}
We do not necessarily need to have as many clusters as classes, but if we want to have a reasonable chance of correctly predicting class labels from clusters, we need at least as many clusters as classes. If we have more clusters, it is then not a problem to have more than one cluster map to the same class. These may for instance correspond to further subdivisions of flower types. 

We experimented with 6 clusters, twice as many as the classes. What happened is that rules satisfying our previously stated constraints were only found for 3 of the clusters: 1, 2 and 5, whereas we do not find any rules for 3, 4 and 6, regardless of the minimum confidence we set. The latter clusters also contained points, but just a few so that the minimum confidence was never satisfied, because most other points belonging to the same classes were in the larger clusters. From this we can conclude that the classes are well separable and that 3 clusters are good enough to obtain an accurate classification. The following rules we found cover each of the clusters.

\begin{table}[]
\centering
\caption{Association rules}
\label{my-label}
\begin{tabular}{llll}
\hline
Parameters & Cluster & Occurences & Confidience \\ \hline
SW='(2.8-3.6]' PL='(4.93-inf)' &\vline 2 &\vline26 &\vline0.93 \\ \hline
SW='(2.8-3.6]' PL='(2.96-4.93]' &\vline 1 &\vline 22 &\vline 0.92 \\ \hline 
SW='(2.8-3.6]' PL='(2.96-4.93]' PW='(0.9-1.7]' &\vline 1 &\vline 19  &\vline 0.9 \\ \hline
\end{tabular}
\end{table}
Predictably, if we change the minimum support to 0.01, more of the clusters get rules, with 1, 2, 3, 5, 6 obtaining high confidence rules, as follows.

\begin{table}[]
\centering
\caption{Association rules}
\label{my-label}
\begin{tabular}{llll}
\hline
Parameters & Cluster & Occurences & Confidience \\ \hline
SL='(5.5-6.7]' PL='(2.96-4.93]' &\vline 1&\vline 39 &\vline 1 \\ \hline 
SL='(5.5-6.7]' PW='(0.9-1.7]' &\vline 1 &\vline 38 &\vline 1 \\ \hline 
PL='(4.93-inf)' PW='(1.7-inf)'&\vline 2 &\vline40  &\vline 1 \\ \hline  
SW='(3.6-inf)' PL='(-inf-2.96]' &\vline 3 &\vline13 &\vline 1\\ \hline 
SW='(2.8-3.6]' PL='(-inf-2.96]' &\vline 5 &\vline36 &\vline 1 \\ \hline 
SW='(2.8-3.6]' PW='(-inf-0.9]' &\vline 5 &\vline36  &\vline 1 \\ \hline 
SL='(-inf-5.5]' SW='(-inf-2.8]' PL='(2.96-4.93]' &\vline 6 &\vline 11 &\vline 1\\ \hline 
\end{tabular}
\end{table}

\end{document}
